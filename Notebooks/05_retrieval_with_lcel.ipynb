{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv('../application/.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.schema.messages import get_buffer_string\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import format_document\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "host = os.getenv(\"PG_VECTOR_HOST\")\n",
    "user = os.getenv(\"PG_VECTOR_USER\")\n",
    "password = os.getenv(\"PG_VECTOR_PASSWORD\")\n",
    "COLLECTION_NAME = os.getenv(\"PGDATABASE\")\n",
    "CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}:5432/{COLLECTION_NAME}\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "store = PGVector(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "retriever = store.as_retriever()\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Monday to Thursday: 11:00 AM - 11:00 PM Friday: 11:00 AM - 12:00 AM (midnight) Saturday: 10:00 AM - 12:00 AM (midnight) Sunday: 10:00 AM - 11:00 PM Special Hours: Our kitchen closes 30 minutes before', metadata={'source': 'opening_hours.txt'}),\n",
       " Document(page_content='the exact amount before confirming your order. Restaurant Opening Hours:', metadata={'source': 'opening_hours.txt'}),\n",
       " Document(page_content='La Tavola Calda - Delivery Service & Opening Hours', metadata={'source': 'opening_hours.txt'}),\n",
       " Document(page_content=\"30 minutes before the restaurant closing time. Whether you're craving a quick lunch, planning a cozy dinner at home, or simply indulging in a late-night snack, La Tavola Calda is just a chat away.\", metadata={'source': 'opening_hours.txt'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"When are the opening hours?\")\n",
    "# The retriever interface provides a more standardized interface to get documents from the vector store, and it's got the method get relevant documents.  \n",
    "# So now let's use the retriever and we want to run the get relevant documents method. We ask when are the opening hours. This is inside our opening hours dot txt.  \n",
    "# So we should be able to retrieve the correct chunks that we stored into the vector store.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "### Line 31-39: Created a regular PromptTemplate named CONDENSE_QUESTION_PROMPT which takes chat history and a follow-up question as input and instructs the language model \n",
    "### to rephrase the follow-up question into a standalone one.\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "### Line 42-49: Created a ChatPromptTemplate named ANSWER_PROMPT which takes context and a question as input and instructs the language model to answer the question based \n",
    "### on that context.\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "### Line 51-52: Creates a simple PromptTemplate named DEFAULT_DOCUMENT_PROMPT which just formats a document for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Building a Data Processing Pipeline:\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Building a Data Processing Pipeline:\n",
    "\n",
    "_inputs = RunnableParallel(\n",
    "    standalone_question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser(),\n",
    ")\n",
    "### Line 65-74: Defines a RunnableParallel named _inputs which performs several tasks in parallel:\n",
    "### Extracts chat history from the input dictionary.\n",
    "### Passes the chat history and question through the CONDENSE_QUESTION_PROMPT to rephrase the question.\n",
    "### Sends the rephrased question to the language model with a temperature of 0 (for more predictable outputs).\n",
    "### Extracts the text response from the language model using StrOutputParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6. Preparing context for answer generation:\n",
    "\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "### Line 77-82: Defines a dictionary named _context which holds two keys:\n",
    "### context: This uses a chain of operations (itemgetter to extract the rephrased question, the retriever to find relevant documents, and _combine_documents to process them) to create the answer context.\n",
    "### question: This simply extracts the question from the input dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | ChatOpenAI() | StrOutputParser()\n",
    "### Line 84-86: Creates a chain named conversational_qa_chain by connecting the various components:\n",
    "### \t• _inputs for processing the input query.\n",
    "### \t• _context to prepare the answer context.\n",
    "### \t• ANSWER_PROMPT to guide the language model with context and question.\n",
    "### \t• The language model (ChatOpenAI) to generate the answer.\n",
    "### StrOutputParser to extract the text response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate Output: {'standalone_question': 'What are the opening hours?'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The restaurant's opening hours are as follows:\\nMonday to Thursday: 11:00 AM - 11:00 PM\\nFriday: 11:00 AM - 12:00 AM (midnight)\\nSaturday: 10:00 AM - 12:00 AM (midnight)\\nSunday: 10:00 AM - 11:00 PM\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_qa_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"When are the opening hours?\",\n",
    "        \"chat_history\": [],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
